<!DOCTYPE html>
<!--[if lte IE 8 ]>
<html class="ie" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<!--
***************  *      *     *
      8          *    *       *
      8          *  *         *
      8          **           *
      8          *  *         *
      8          *    *       *
      8          *      *     *
      8          *        *   ***********    -----Theme By Kieran(http://go.kieran.top)
-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<!--<![endif]-->

<head>
  <title>【爬虫系列】3、基本使用 | 方糖~</title>
  <!-- Meta data -->
    <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="generator" content="方糖~">
    <meta name="author" content="Yu Fangheng">
    <meta name="description" content="Henry's Blog~" />
    <meta name="keywords" content="" />

    <!-- Favicon, (keep icon in root folder) -->
    <link rel="Shortcut Icon" href="/HenryBalthier.github.io/img/favicon.ico" type="image/ico">

    <link rel="alternate" href="/atom.xml" title="方糖~" type="application/atom+xml">
    <link rel="stylesheet" href="/HenryBalthier.github.io/css/all.css" media="screen" type="text/css">
    
    <link rel="stylesheet" href="/HenryBalthier.github.io/highlightjs/vs.css" type="text/css">
    

    <!--[if IE 8]>
    <link rel="stylesheet" type="text/css" href="/HenryBalthier.github.io/css/ie8.css" />
    <![endif]-->

    <!-- jQuery | Load our jQuery, with an alternative source fallback to a local version if request is unavailable -->
    <script src="/HenryBalthier.github.io/js/jquery-1.11.1.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/jquery-1.11.1.min.js"><\/script>')</script>

    <!-- Load these in the <head> for quicker IE8+ load times -->
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="/HenryBalthier.github.io/js/html5shiv.min.js"></script>
    <script src="/HenryBalthier.github.io/js/respond.min.js"></script>
    <![endif]-->

  
  
  

  <style>.col-md-8.col-md-offset-2.opening-statement img{display:none;}</style>
</head>

<!--
<body class="post-template">
-->
<body id="index" class="lightnav animsition">

      <!-- ============================ Off-canvas navigation =========================== -->

    <div class="sb-slidebar sb-right sb-style-overlay sb-momentum-scrolling">
        <div class="sb-close" aria-label="Close Menu" aria-hidden="true">
            <img src="/HenryBalthier.github.io/img/close.png" alt="Close"/>
        </div>
        <!-- Lists in Slidebars -->
        <ul class="sb-menu">
            <li><a href="/HenryBalthier.github.io/" class="animsition-link" title="Home">Home</a></li>
            <li><a href="/HenryBalthier.github.io/archives" class="animsition-link" title="archive">archives</a></li>
            <!-- Dropdown Menu -->
			 
            <li>
                <a class="sb-toggle-submenu">Works<span class="sb-caret"></span></a>
                <ul class="sb-submenu">
                    
                        <li><a href="https://henrybalthier.github.io/HenryBalthier.github.io/tags/python/" target="_BLANK" class="animsition-link">Python</a></li>
                    
                        <li><a href="https://henrybalthier.github.io/HenryBalthier.github.io/tags/Linux/" target="_BLANK" class="animsition-link">Linux</a></li>
                    
                        <li><a href="https://henrybalthier.github.io/HenryBalthier.github.io/tags/Java/" target="_BLANK" class="animsition-link">Java</a></li>
                    
                        <li><a href="https://henrybalthier.github.io/HenryBalthier.github.io/tags/%E7%AE%97%E6%B3%95/" target="_BLANK" class="animsition-link">算法</a></li>
                    
                </ul>
            </li>
            
            
            
        </ul>
        <!-- Lists in Slidebars -->
        <ul class="sb-menu secondary">
            <li><a href="/HenryBalthier.github.io/about.html" class="animsition-link" title="about">About</a></li>
            <li><a href="/HenryBalthier.github.io/atom.xml" class="animsition-link" title="rss">RSS</a></li>
        </ul>
    </div>
    
    <!-- ============================ END Off-canvas navigation =========================== -->

    <!-- ============================ #sb-site Main Page Wrapper =========================== -->

    <div id="sb-site">
        <!-- #sb-site - All page content should be contained within this id, except the off-canvas navigation itself -->

        <!-- ============================ Header & Logo bar =========================== -->

        <div id="navigation" class="navbar navbar-fixed-top">
            <div class="navbar-inner">
                <div class="container">
                    <!-- Nav logo -->
                    <div class="logo">
                        <a href="/" title="Logo" class="animsition-link">
                         <img src="/img/logo.png" alt="Logo" width="35px;"/> 
                        </a>
                    </div>
                    <!-- // Nav logo -->
                    <!-- Info-bar -->
                    <nav>
                        <ul class="nav">
                            <li><a href="/" class="animsition-link">方糖~</a></li>
                            <li class="nolink"><span>Always </span>Creative.</li>
                            
                            <li><a href="https://github.com/" title="Github" target="_blank"><i class="icon-github"></i></a></li>
                            
                            
                            <li><a href="https://twitter.com/" title="Twitter" target="_blank"><i class="icon-twitter"></i></a></li>
                            
                            
                            <li><a href="https://www.facebook.com/" title="Facebook" target="_blank"><i class="icon-facebook"></i></a></li>
                            
                            
                            <li><a href="https://google.com/" title="Google-Plus" target="_blank"><i class="icon-google-plus"></i></a></li>
                            
                            
                            <li><a href="http://weibo.com/" title="Sina-Weibo" target="_blank"><i class="icon-sina-weibo"></i></a></li>
                            
                            <li class="nolink"><span>Welcome!</span></li>
                        </ul>
                    </nav>
                    <!--// Info-bar -->
                </div>
                <!-- // .container -->
                <div class="learnmore sb-toggle-right">More</div>
                <button type="button" class="navbar-toggle menu-icon sb-toggle-right" title="More">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar before"></span>
                <span class="icon-bar main"></span>
                <span class="icon-bar after"></span>
                </button>
            </div>
            <!-- // .navbar-inner -->
        </div>

        <!-- ============================ Header & Logo bar =========================== -->


      
<section id="intro">
    <div class="container">
        <div class="row col-md-offset-2">
            <div class="col-md-8">
    			<span class="post-meta">
      <time datetime="2017-08-29T08:29:55.000Z" itemprop="datePublished">
          2017-08-29
      </time>
    
    
    | 
    <a href='/HenryBalthier.github.io/tags/爬虫/'>爬虫</a>
    
    
</span>
                <h1>【爬虫系列】3、基本使用</h1>
            </div>
        </div>
        <div class="col-md-8 col-md-offset-2">
      		<h4 id="主要内容："><a href="#主要内容：" class="headerlink" title="主要内容："></a>主要内容：</h4><ul>
<li>Urllib</li>
<li>Requests</li>
<li>正则表达式</li>
</ul>
<a id="more"></a>
<hr>
<h4 id="爬虫学习参考网站："><a href="#爬虫学习参考网站：" class="headerlink" title="爬虫学习参考网站："></a>爬虫学习参考网站：</h4><p><a href="https://germey.gitbooks.io/python3webspider/content/0-%E7%9B%AE%E5%BD%95.html" target="_blank" rel="external">Python3爬虫实战</a></p>
<h2 id="3-1、Urllib使用"><a href="#3-1、Urllib使用" class="headerlink" title="3.1、Urllib使用"></a>3.1、Urllib使用</h2><p>　　在 Python2 版本中，有 Urllib 和 Urlib2 两个库可以用来实现Request的发送。而在 Python3 中，已经不存在 Urllib2 这个库了，统一为 Urllib，其官方文档链接为：<a href="https://docs.python.org/3/library/urllib.html" target="_blank" rel="external">https://docs.python.org/3/library/urllib.html</a></p>
<p>　　我们首先了解一下 Urllib 库，它是 Python 内置的 HTTP 请求库，也就是说我们不需要额外安装即可使用，它包含四个模块：</p>
<ul>
<li>第一个模块 request，它是最基本的 HTTP 请求模块，我们可以用它来模拟发送一请求，就像在浏览器里输入网址然后敲击回车一样，只需要给库方法传入 URL 还有额外的参数，就可以模拟实现这个过程了。</li>
<li>第二个 error 模块即异常处理模块，如果出现请求错误，我们可以捕获这些异常，然后进行重试或其他操作保证程序不会意外终止。</li>
<li>第三个 parse 模块是一个工具模块，提供了许多 URL 处理方法，比如拆分、解析、合并等等的方法。</li>
<li>第四个模块是 robotparser，主要是用来识别网站的 robots.txt 文件，然后判断哪些网站可以爬，哪些网站不可以爬的，其实用的比较少。</li>
</ul>
<h4 id="3-1-1、发送请求"><a href="#3-1-1、发送请求" class="headerlink" title="3.1.1、发送请求"></a>3.1.1、发送请求</h4><p><strong>1. urlopen()</strong></p>
<p>　　urllib.request 模块提供了最基本的构造 HTTP 请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有处理authenticaton（授权验证），redirections（重定向)，cookies（浏览器Cookies）以及其它内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib.request</div><div class="line"></div><div class="line">response = urllib.request.urlopen(<span class="string">'https://www.python.org'</span>)</div><div class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</div></pre></td></tr></table></figure>
<p>　　它是一个 HTTPResposne 类型的对象，它主要包含的方法有 read()、readinto()、getheader(name)、getheaders()、fileno() 等方法和 msg、version、status、reason、debuglevel、closed 等属性。</p>
<p>　　得到这个对象之后，我们把它赋值为 response 变量，然后就可以调用这些方法和属性，得到返回结果的一系列信息了。</p>
<p>　　例如调用 read() 方法可以得到返回的网页内容，调用 status 属性就可以得到返回结果的状态码</p>
<p><strong>data参数</strong></p>
<p>　　data 参数是可选的，如果要添加 data，它要是字节流编码格式的内容，即 bytes 类型，通过 bytes() 方法可以进行转化，另外如果传递了这个 data 参数，它的请求方式就不再是 GET 方式请求，而是 POST。</p>
<p><strong>timeout参数</strong></p>
<p>　　timeout 参数可以设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间还没有得到响应，就会抛出异常，如果不指定，就会使用全局默认时间。它支持 HTTP、HTTPS、FTP 请求。</p>
<p><strong>其他参数</strong></p>
<p>　　还有 context 参数，它必须是 ssl.SSLContext 类型，用来指定 SSL 设置。</p>
<p>　　cafile 和 capath 两个参数是指定 CA 证书和它的路径，这个在请求 HTTPS 链接时会有用。</p>
<p>　　cadefault 参数现在已经弃用了，默认为 False。</p>
<p><strong>2. Request</strong></p>
<p>　　由上我们知道利用 urlopen() 方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入 Headers 等信息，我们就可以利用更强大的 Request 类来构建一个请求。</p>
<p>　　首先我们用一个实例来感受一下 Request 的用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib.request</div><div class="line"></div><div class="line">request = urllib.request.Request(<span class="string">'https://python.org'</span>)</div><div class="line">response = urllib.request.urlopen(request)</div><div class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</div></pre></td></tr></table></figure>
<p>　　可以发现，我们依然是用 urlopen() 方法来发送这个请求，只不过这次 urlopen() 方法的参数不再是一个 URL，而是一个 Request 类型的对象，通过构造这个这个数据结构，一方面我们可以将请求独立成一个对象，另一方面可配置参数更加丰富和灵活。</p>
<p>　　第一个 url 参数是请求 URL，这个是必传参数，其他的都是可选参数。</p>
<p>　　第二个 data 参数如果要传必须传 bytes（字节流）类型的，如果是一个字典，可以先用 urllib.parse 模块里的 urlencode() 编码。</p>
<p>　　第三个 headers 参数是一个字典，这个就是 Request Headers 了，你可以在构造 Request 时通过 headers 参数直接构造，也可以通过调用 Request 实例的 add_header() 方法来添加。</p>
<p>　　添加 Request Headers 最常用的用法就是通过修改 User-Agent 来伪装浏览器，默认的 User-Agent 是 Python-urllib，我们可以通过修改它来伪装浏览器，比如要伪装火狐浏览器，你可以把它设置为：</p>
<pre><code>Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11
</code></pre><p>　　第四个 origin_req_host 参数指的是请求方的 host 名称或者 IP 地址。</p>
<p>　　第五个 unverifiable 参数指的是这个请求是否是无法验证的，默认是False。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个 HTML 文档中的图片，但是我们没有自动抓取图像的权限，这时 unverifiable 的值就是 True。</p>
<p>　　第六个 method 参数是一个字符串，它用来指示请求使用的方法，比如GET，POST，PUT等等。</p>
<p><strong>3. 高级用法</strong></p>
<p>　　有没有发现，在上面的过程中，我们虽然可以构造 Request，但是一些更高级的操作，比如 Cookies 处理，代理设置等操作我们该怎么办？</p>
<p>　　接下来就需要更强大的工具 Handler 登场了。</p>
<p>　　简而言之我们可以把它理解为各种处理器，有专门处理登录验证的，有处理 Cookies 的，有处理代理设置的，利用它们我们几乎可以做到任何 HTTP 请求中所有的事情。</p>
<p>　　首先介绍下 urllib.request 模块里的 BaseHandler类，它是所有其他 Handler 的父类，它提供了最基本的 Handler 的方法，例如 default_open()、protocol_request() 方法等。</p>
<p>　　接下来就有各种 Handler 子类继承这个 BaseHandler 类，举例几个如下：</p>
<ul>
<li>HTTPDefaultErrorHandler 用于处理 HTTP 响应错误，错误都会抛出 HTTPError 类型的异常。</li>
<li>HTTPRedirectHandler 用于处理重定向。</li>
<li>HTTPCookieProcessor 用于处理 Cookies。</li>
<li>ProxyHandler 用于设置代理，默认代理为空。</li>
<li>HTTPPasswordMgr 用于管理密码，它维护了用户名密码的表。</li>
<li>HTTPBasicAuthHandler 用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。</li>
<li>另外还有其他的 Handler 类，在这不一一列举了，详情可以参考官方文档： <a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler" target="_blank" rel="external">https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler</a></li>
</ul>
<p>　　它们怎么来使用，不用着急，下面会有实例为你演示。</p>
<p>　　另外一个比较重要的类就是 OpenerDirector，我们可以称之为 Opener，我们之前用过 urlopen() 这个方法，实际上它就是 Urllib为我们提供的一个 Opener。</p>
<p>　　那么为什么要引入 Opener 呢？因为我们需要实现更高级的功能，之前我们使用的 Request、urlopen() 相当于类库为你封装好了极其常用的请求方法，利用它们两个我们就可以完成基本的请求，但是现在不一样了，我们需要实现更高级的功能，所以我们需要深入一层进行配置，使用更底层的实例来完成我们的操作。</p>
<p>　　所以，在这里我们就用到了比调用 urlopen() 的对象的更普遍的对象，也就是 Opener。</p>
<p>　　Opener 可以使用 open() 方法，返回的类型和 urlopen() 如出一辙。那么它和 Handler 有什么关系？简而言之，就是利用 Handler 来构建 Opener。</p>
<h4 id="3-1-2、处理异常"><a href="#3-1-2、处理异常" class="headerlink" title="3.1.2、处理异常"></a>3.1.2、处理异常</h4><p><strong>1. URLError</strong></p>
<p>　　URLError 类来自 Urllib 库的 error 模块，它继承自 OSError 类，是 error 异常模块的基类，由 request 模块生的异常都可以通过捕获这个类来处理。</p>
<p>　　它具有一个属性 reason，即返回错误的原因。</p>
<p>　　下面用一个实例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</div><div class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</div><div class="line">    print(e.reason)</div></pre></td></tr></table></figure>
<p>　　我们打开一个不存在的页面，照理来说应该会报错，但是这时我们捕获了 URLError 这个异常，运行结果如下：</p>
<pre><code>Not Found
</code></pre><p>　　程序没有直接报错，而是输出了如上内容，这样通过如上操作，我们就可以避免程序异常终止，同时异常得到了有效处理。</p>
<p><strong>2. HTTPError</strong></p>
<p>　　它是 URLError 的子类，专门用来处理 HTTP 请求错误，比如认证请求失败等等。</p>
<p>　　它有三个属性。</p>
<ul>
<li>code，返回 HTTP Status Code，即状态码，比如 404 网页不存在，500 服务器内部错误等等。</li>
<li>reason，同父类一样，返回错误的原因。</li>
<li>headers，返回 Request Headers。</li>
</ul>
<h4 id="3-1-3、解析链接"><a href="#3-1-3、解析链接" class="headerlink" title="3.1.3、解析链接"></a>3.1.3、解析链接</h4><p>　　rllib 库里还提供了 parse 这个模块，它定义了处理 URL 的标准接口，例如实现 URL 各部分的抽取，合并以及链接转换。它支持如下协议的 URL 处理：file、ftp、gopher、hdl、http、https、imap、mailto、 mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、shttp、 sip、sips、snews、svn、svn+ssh、telnet、wais。</p>
<p><strong>1. urlparse()</strong></p>
<p>　　urlparse() 方法可以实现 URL 的识别和分段，我们先用一个实例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</div><div class="line"></div><div class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</div><div class="line">print(type(result), result)</div></pre></td></tr></table></figure>
<p>　　在这里我们利用了 urlparse() 方法进行了一个 URL 的解析，首先输出了解析结果的类型，然后将结果也输出出来。</p>
<p>　　运行结果：</p>
<pre><code>&lt;class &apos;urllib.parse.ParseResult&apos;&gt;
ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;)
</code></pre><p>　　观察可以看到，返回结果是一个 ParseResult 类型的对象，它包含了六个部分，分别是 scheme、netloc、path、params、query、fragment。</p>
<p>　　观察一下实例的URL：</p>
<pre><code>http://www.baidu.com/index.html;user?id=5#comment
</code></pre><p>　　urlparse() 方法将其拆分成了六部分，大体观察可以发现，解析时有特定的分隔符，比如 :// 前面的就是 scheme，代表协议，第一个 / 前面便是 netloc，即域名，分号 ; 前面是 params，代表参数。</p>
<p>　　所以可以得出一个标准的链接格式如下：</p>
<pre><code>scheme://netloc/path;parameters?query#fragment
</code></pre><p>　　一个标准的 URL 都会符合这个规则，利用 urlparse() 方法我们可以将它解析拆分开来。</p>
<p>　　除了这种最基本的解析方式，urlopen() 方法还有其他配置吗？接下来看一下它的 API 用法：</p>
<pre><code>urllib.parse.urlparse(urlstring, scheme=&apos;&apos;, allow_fragments=True)
</code></pre><p>　　可以看到它有三个参数：</p>
<ul>
<li>urlstring，是必填项，即待解析的 URL。</li>
<li>scheme，是默认的协议（比如http、https等），假如这个链接没有带协议信息，会将这个作为默认的协议。</li>
</ul>
<p><strong>2. urlunparse()</strong></p>
<p>　　有了 urlparse() 那相应地就有了它的对立方法 urlunparse()。</p>
<p>　　它接受的参数是一个可迭代对象，但是它的长度必须是 6，否则会抛出参数数量不足或者过多的问题。</p>
<p>　　先用一个实例感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</div><div class="line"></div><div class="line">data = [<span class="string">'http'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'user'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</div><div class="line">print(urlunparse(data))</div></pre></td></tr></table></figure>
<p>　　参数 data 用了列表类型，当然你也可以用其他的类型如元组或者特定的数据结构。</p>
<p>　　运行结果如下：</p>
<pre><code>http://www.baidu.com/index.html;user?a=6#comment
</code></pre><p>　　这样我们就成功实现了 URL 的构造。</p>
<p><strong>3. urlsplit()</strong></p>
<p>　　这个和 urlparse() 方法非常相似，只不过它不会单独解析 parameters 这一部分，只返回五个结果。上面例子中的 parameters 会合并到 path中，用一个实例感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</div><div class="line"></div><div class="line">result = urlsplit(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</div><div class="line">print(result)</div></pre></td></tr></table></figure>
<p>　　运行结果：</p>
<pre><code>SplitResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;)
</code></pre><p>　　可以发现返回结果是 SplitResult，其实也是一个元组类型，可以用属性获取值也可以用索引来获取</p>
<p><strong>4. urlunsplit()</strong></p>
<p>　　与 urlunparse() 类似，也是将链接的各个部分组合成完整链接的方法，传入的也是一个可迭代对象，例如列表、元组等等，唯一的区别是，长度必须为 5。</p>
<p>　　用一个实例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunsplit</div><div class="line"></div><div class="line">data = [<span class="string">'http'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</div><div class="line">print(urlunsplit(data))</div></pre></td></tr></table></figure>
<p>　　运行结果：</p>
<pre><code>http://www.baidu.com/index.html?a=6#comment
</code></pre><p><strong>5. urljoin()</strong></p>
<p>　　有了 urlunparse() 和 urlunsplit() 方法，我们可以完成链接的合并，不过前提必须要有特定长度的对象，链接的每一部分都要清晰分开。</p>
<p>　　生成链接还有另一个方法，利用 urljoin() 方法我们可以提供一个 base_url（基础链接），新的链接作为第二个参数，方法会分析 base_url 的 scheme、netloc、path 这三个内容对新链接缺失的部分进行补充，作为结果返回。</p>
<p><strong>6. urlencode()</strong></p>
<p><strong>7. parse_qs()</strong></p>
<p><strong>8. parse_qsl()</strong></p>
<p><strong>9. quote()</strong></p>
<p><strong>10. unquote()</strong></p>
<h4 id="3-1-4、Robots协议"><a href="#3-1-4、Robots协议" class="headerlink" title="3.1.4、Robots协议"></a>3.1.4、Robots协议</h4><p>　　Robots 协议也被称作爬虫协议、机器人协议，它的全名叫做网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫做 robots.txt 的文本文件，放在网站的根目录下。</p>
<p>　　当搜索爬虫访问一个站点时，它首先会检查下这个站点根目录下是否存在 robots.txt 文件，如果存在，搜索爬虫会根据其中定义的爬取范围来爬取。如果没有找到这个文件，那么搜索爬虫便会访问所有可直接访问的页面。</p>
<p>　　下面我们看一个 robots.txt 的样例：</p>
<pre><code>User-agent: *
Disallow: /
Allow: /public/
</code></pre><p>　　以上的两行实现了对所有搜索爬虫只允许爬取 public目录的作用。</p>
<p>　　如上简单的两行，保存成 robots.txt 文件，放在网站的根目录下，和网站的入口文件放在一起。比如 index.php、index.html、index.jsp 等等。</p>
<p>　　那么上面的 User-agent 就描述了搜索爬虫的名称，在这里将值设置为 *，则代表该协议对任何的爬取爬虫有效。比如我们可以设置：</p>
<pre><code>User-agent: Baiduspider
</code></pre><p>　　这就代表我们设置的规则对百度爬虫是有效的。如果有多条 User-agent 记录，则就会有多个爬虫会受到爬取限制，但至少需要指定一条。</p>
<p>　　Disallow 指定了不允许抓取的目录，比如上述例子中设置为/则代表不允许抓取所有页面。</p>
<p>　　Allow 一般和 Disallow 一起使用，一般不会单独使用，用来排除某些限制，现在我们设置为 /public/ ，起到的作用是所有页面不允许抓取，但是 public 目录是可以抓取的。</p>
<p><strong>robotparser</strong></p>
<p>　　了解了什么是 Robots 协议之后，我们就可以使用 robotparser 模块来解析 robots.txt 了。</p>
<p>　　robotparser 模块提供了一个类，叫做 RobotFileParser。它可以根据某网站的 robots.txt 文件来判断一个爬取爬虫是否有权限来爬取这个网页。</p>
<p>　　使用非常简单，首先看一下它的声明</p>
<pre><code>urllib.robotparser.RobotFileParser(url=&apos;&apos;)
</code></pre><p>　　使用这个类的时候非常简单，只需要在构造方法里传入 robots.txt的链接即可。当然也可以声明时不传入，默认为空，再使用 set_url() 方法设置一下也可以。</p>
<p>　　有常用的几个方法分别介绍一下：</p>
<ul>
<li>set_url()，用来设置 robots.txt 文件的链接。如果已经在创建 RobotFileParser 对象时传入了链接，那就不需要再使用这个方法设置了。</li>
<li>read()，读取 robots.txt 文件并进行分析，注意这个函数是执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 False，所以一定记得调用这个方法，这个方法不会返回任何内容，但是执行了读取操作。</li>
<li>parse()，用来解析 robots.txt 文件，传入的参数是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容。</li>
<li>can_fetch()，方法传入两个参数，第一个是 User-agent，第二个是要抓取的 URL，返回的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 True 或 False。</li>
<li>mtime()，返回的是上次抓取和分析 robots.txt 的时间，这个对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的 robots.txt。</li>
<li>modified()，同样的对于长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析 robots.txt 的时间。</li>
</ul>
<hr>
<h2 id="3-2、Requests使用"><a href="#3-2、Requests使用" class="headerlink" title="3.2、Requests使用"></a>3.2、Requests使用</h2><p>　　在 Urllib 库中有 urlopen() 的方法，实际上它是以 GET 方式请求了一个网页。</p>
<p>　　那么在 Requests 中，相应的方法就是 get() 方法，是不是感觉表达更明确一些？</p>
<p>　　下面我们用一个实例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">'https://www.baidu.com/'</span>)</div><div class="line">print(type(r))</div><div class="line">print(r.status_code)</div><div class="line">print(type(r.text))</div><div class="line">print(r.text)</div><div class="line">print(r.cookies)</div></pre></td></tr></table></figure>
<p>　　运行结果如下：</p>
<pre><code>&lt;class &apos;requests.models.Response&apos;&gt;
200
&lt;class &apos;str&apos;&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;script&gt;
        location.replace(location.href.replace(&quot;https://&quot;,&quot;http://&quot;));
    &lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;noscript&gt;&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0;url=http://www.baidu.com/&quot;&gt;&lt;/noscript&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;RequestsCookieJar[&lt;Cookie BIDUPSID=992C3B26F4C4D09505C5E959D5FBC005 for .baidu.com/&gt;, &lt;Cookie PSTM=1472227535 for .baidu.com/&gt;, &lt;Cookie __bsi=15304754498609545148_00_40_N_N_2_0303_C02F_N_N_N_0 for .www.baidu.com/&gt;, &lt;Cookie BD_NOT_HTTPS=1 for www.baidu.com/&gt;]&gt;
</code></pre><p>　　上面的例子中我们调用 get() 方法即可实现和 urlopen() 相同的操作，得到一个 Response 对象，然后分别输出了 Response 的类型，Status Code，Response Body 的类型、内容还有 Cookies。</p>
<p>　　通过上述实例可以发现，它的返回类型是 requests.models.Response，Response Body 的类型是字符串 str，Cookies 的类型是 RequestsCookieJar。</p>
<p>　　使用了 get() 方法就成功实现了一个 GET 请求，但这倒不算什么，更方便的在于其他的请求类型依然可以用一句话来完成。
　　</p>
<h4 id="3-2-1、GET请求"><a href="#3-2-1、GET请求" class="headerlink" title="3.2.1、GET请求"></a>3.2.1、GET请求</h4><p>　　HTTP 中最常见的请求之一就是 GET 请求</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>)</div><div class="line">print(type(r.text))</div><div class="line">print(r.json())</div><div class="line">print(type(r.json()))</div></pre></td></tr></table></figure>
<p><strong>抓取网页</strong></p>
<p>　　如上的请求链接返回的是 Json 形式的字符串，那么如果我们请求普通的网页，那么肯定就能获得相应的内容了。</p>
<p>　　下面我们以知乎－发现页面为例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">headers = &#123;</div><div class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'</span></div><div class="line">&#125;</div><div class="line">r = requests.get(<span class="string">"https://www.zhihu.com/explore"</span>, headers=headers)</div><div class="line">pattern = re.compile(<span class="string">'explore-feed.*?question_link.*?&gt;(.*?)&lt;/a&gt;'</span>, re.S)</div><div class="line">titles = re.findall(pattern, r.text)</div><div class="line">print(titles)</div></pre></td></tr></table></figure>
<p>　　如上代码，我们请求了知乎－发现页面：<a href="https://www.zhihu.com/explore，在这里加入了" target="_blank" rel="external">https://www.zhihu.com/explore，在这里加入了</a> Headers 信息，其中包含了 User-Agent 字段信息，也就是浏览器标识信息。如果不加这个，知乎会禁止抓取。</p>
<p>　　在接下来用到了最基础的正则表达式，来匹配出所有的问题内容，关于正则表达式会在后面的章节中详细介绍，在这里作为用到实例来配合讲解。</p>
<p>　　运行结果如下：</p>
<pre><code>[&apos;\n为什么很多人喜欢提及「拉丁语系」这个词？\n&apos;, &apos;\n在没有水的情况下水系宝可梦如何战斗？\n&apos;, &apos;\n有哪些经验可以送给 Kindle 新人？\n&apos;, &apos;\n谷歌的广告业务是如何赚钱的？\n&apos;, &apos;\n程序员该学习什么，能在上学期间挣钱？\n&apos;, &apos;\n有哪些原本只是一个小消息，但回看发现是个惊天大新闻的例子？\n&apos;, &apos;\n如何评价今敏？\n&apos;, &apos;\n源氏是怎么把那么长的刀从背后拔出来的？\n&apos;, &apos;\n年轻时得了绝症或大病是怎样的感受？\n&apos;, &apos;\n年轻时得了绝症或大病是怎样的感受？\n&apos;]
</code></pre><p>　　发现成功提取出了所有的问题内容。</p>
<p><strong>抓取二进制数据</strong></p>
<p>　　在上面的例子中，我们抓取的是知乎的一个页面，实际上它返回的是一个 HTML 文档，那么如果我们想抓去图片、音频、视频等文件的话应该怎么办呢？</p>
<p>　　我们都知道，图片、音频、视频这些文件都是本质上由二进制码组成的，由于有特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。所以想要抓取他们，那就需要拿到他们的二进制码。</p>
<p>　　下面我们以 GitHub 的站点图标为例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">"https://github.com/favicon.ico"</span>)</div><div class="line">print(r.text)</div><div class="line">print(r.content)</div></pre></td></tr></table></figure>
<p>　　抓取的内容是站点图标，也就是在浏览器每一个标签上显示的小图标，</p>
<p>　　进一步地，我们可以将刚才提取到的图片保存下来。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">"https://github.com/favicon.ico"</span>)</div><div class="line"><span class="keyword">with</span> open(<span class="string">'favicon.ico'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(r.content)</div></pre></td></tr></table></figure></p>
<p>　　在这里用了 open() 方法，第一个参数是文件名称，第二个参数代表以二进制写的形式打开，可以向文件里写入二进制数据，然后保存。</p>
<p>　　运行结束之后，可以发现在文件夹中出现了名为 favicon.ico 的图标</p>
<p><strong>添加Headers</strong></p>
<p>　　如 urllib.request 一样，我们也可以通过 headers 参数来传递头信息。</p>
<p>　　比如上面的知乎的例子，如果不传递 Headers，就不能正常请求：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">"https://www.zhihu.com/explore"</span>)</div><div class="line">print(r.text)</div></pre></td></tr></table></figure></p>
<p>　　运行结果如下：</p>
<pre><code>&lt;html&gt;&lt;body&gt;&lt;h1&gt;500 Server Error&lt;/h1&gt;
An internal server error occured.
&lt;/body&gt;&lt;/html&gt;
</code></pre><p>　　但如果加上 Headers 中加上 User-Agent 信息，那就没问题了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">headers = &#123;</div><div class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'</span></div><div class="line">&#125;</div><div class="line">r = requests.get(<span class="string">"https://www.zhihu.com/explore"</span>, headers=headers)</div><div class="line">print(r.text)</div></pre></td></tr></table></figure></p>
<p>　　当然我们可以在 headers 这个参数中任意添加其他的字段信息。</p>
<h4 id="3-2-2、POST请求"><a href="#3-2-2、POST请求" class="headerlink" title="3.2.2、POST请求"></a>3.2.2、POST请求</h4><p>　　在前面我们了解了最基本的 GET 请求，另外一种比较常见的请求方式就是 POST 了，就像模拟表单提交一样，将一些数据提交到某个链接。</p>
<p>　　使用 Request 是实现 POST 请求同样非常简单。</p>
<p>　　我们先用一个实例来感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="string">'22'</span>&#125;</div><div class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, data=data)</div><div class="line">print(r.text)</div></pre></td></tr></table></figure></p>
<h4 id="3-2-3、Response"><a href="#3-2-3、Response" class="headerlink" title="3.2.3、Response"></a>3.2.3、Response</h4><p>　　发送 Request 之后，得到的自然就是 Response，在上面的实例中我们使用了 text 和 content 获取了 Response 内容，不过还有很多属性和方法可以获取其他的信息，比如状态码 Status Code、Headers、Cookies 等信息。</p>
<p>　　下面用一个实例来感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">'http://www.jianshu.com'</span>)</div><div class="line">print(type(r.status_code), r.status_code)</div><div class="line">print(type(r.headers), r.headers)</div><div class="line">print(type(r.cookies), r.cookies)</div><div class="line">print(type(r.url), r.url)</div><div class="line">print(type(r.history), r.history)</div></pre></td></tr></table></figure></p>
<p>　　在这里分别打印输出了 status_code 属性得到状态码， headers 属性得到 Response Headers，cookies 属性得到 Cookies，url 属性得到 URL，history 属性得到请求历史。</p>
<h4 id="3-2-4、文件上传"><a href="#3-2-4、文件上传" class="headerlink" title="3.2.4、文件上传"></a>3.2.4、文件上传</h4><p>　　我们知道 Reqeuests 可以模拟提交一些数据，假如有的网站需要我们上传文件，我们同样可以利用它来上传，实现非常简单，实例如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">files = &#123;<span class="string">'file'</span>: open(<span class="string">'favicon.ico'</span>, <span class="string">'rb'</span>)&#125;</div><div class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, files=files)</div><div class="line">print(r.text)</div></pre></td></tr></table></figure></p>
<p>　　在上面一节中我们下载保存了一个文件叫做 favicon.ico，这次我们用它为例来模拟文件上传的过程。需要注意的是，favicon.ico 这个文件需要和当前脚本在同一目录下。如果有其它文件，当然也可以使用其它文件来上传，更改下名称即可。</p>
<h4 id="3-2-5、Cookies"><a href="#3-2-5、Cookies" class="headerlink" title="3.2.5、Cookies"></a>3.2.5、Cookies</h4><p>　　在前面我们使用了 Urllib 处理过 Cookies，写法比较复杂，而有了 Requests，获取和设置 Cookies 只需要一步即可完成。</p>
<p>　　我们先用一个实例感受一下获取 Cookies 的过程：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">"https://www.baidu.com"</span>)</div><div class="line">print(r.cookies)</div><div class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> r.cookies.items():</div><div class="line">    print(key + <span class="string">'='</span> + value)</div></pre></td></tr></table></figure></p>
<p>　　运行结果如下：</p>
<pre><code>&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;, &lt;Cookie __bsi=13533594356813414194_00_14_N_N_2_0303_C02F_N_N_N_0 for .www.baidu.com/&gt;]&gt;
BDORZ=27315
__bsi=13533594356813414194_00_14_N_N_2_0303_C02F_N_N_N_0
</code></pre><p>　　首先我们调用了 cookies 属性即可成功得到了 Cookies，可以发现它是一个 RequestCookieJar 类型，然后我们用 items() 方法将其转化为元组组成的列表，遍历输出每一个 Cookie 的名和值，实现 Cookies 的遍历解析。</p>
<p>　　当然，我们也可以直接用 Cookies 来维持登录状态。</p>
<h4 id="3-2-5、Session会话维持"><a href="#3-2-5、Session会话维持" class="headerlink" title="3.2.5、Session会话维持"></a>3.2.5、Session会话维持</h4><p>　　在 Requests 中，我们如果直接利用 get() 或 post() 等方法的确可以做到模拟网页的请求。但是这实际上是相当于不同的会话，即不同的 Session，也就是说相当于你用了两个浏览器打开了不同的页面。</p>
<p>　　设想这样一个场景，我们第一个请求利用了 post() 方法登录了某个网站，第二次想获取成功登录后的自己的个人信息，你又用了一次 get() 方法去请求个人信息页面。实际上，这相当于打开了两个浏览器，是两个完全不相关的会话，能成功获取个人信息吗？那当然不能。</p>
<p>　　有小伙伴可能就说了，我在两次请求的时候都设置好一样的 Cookies 不就行了？可以，但这样做起来还是显得很繁琐，我们还有更简单的解决方法。</p>
<p>　　其实解决这个问题的主要方法就是维持同一个会话，也就是相当于打开一个新的浏览器选项卡而不是新开一个浏览器。但是我又不想每次设置 Cookies，那该怎么办？这时候就有了新的利器 Session对象。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">s = requests.Session()</div><div class="line">s.get(<span class="string">'http://httpbin.org/cookies/set/number/123456789'</span>)</div><div class="line">r = s.get(<span class="string">'http://httpbin.org/cookies'</span>)</div><div class="line">print(r.text)</div></pre></td></tr></table></figure></p>
<p>　　看下运行结果：</p>
<pre><code>{
  &quot;cookies&quot;: {
    &quot;number&quot;: &quot;123456789&quot;
  }
}
</code></pre><p>　　成功获取！这下能体会到同一个会话和不同会话的区别了吧？</p>
<p>　　所以，利用 Session 我们可以做到模拟同一个会话，而且不用担心 Cookies 的问题，通常用于模拟登录成功之后再进行下一步的操作。</p>
<h4 id="3-2-6、SSL证书验证"><a href="#3-2-6、SSL证书验证" class="headerlink" title="3.2.6、SSL证书验证"></a>3.2.6、SSL证书验证</h4><p>　　Requests 提供了证书验证的功能，当发送 HTTP 请求的时候，它会检查 SSL 证书，我们可以使用 verify 这个参数来控制是否检查此证书，其实如果不加的话默认是 True，会自动验证。</p>
<h4 id="3-2-7、代理设置"><a href="#3-2-7、代理设置" class="headerlink" title="3.2.7、代理设置"></a>3.2.7、代理设置</h4><p>　　对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会直接登录验证，验证码，甚至直接把IP给封禁掉。</p>
<p>　　那么为了防止这种情况的发生，我们就需要设置代理来解决这个问题，在 Requests 中需要用到 proxies 这个参数。</p>
<p>　　可以用这样的方式设置：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">proxies = &#123;</div><div class="line">  <span class="string">"http"</span>: <span class="string">"http://10.10.1.10:3128"</span>,</div><div class="line">  <span class="string">"https"</span>: <span class="string">"http://10.10.1.10:1080"</span>,</div><div class="line">&#125;</div><div class="line"></div><div class="line">requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</div></pre></td></tr></table></figure></p>
<h4 id="3-2-8、超时设置"><a href="#3-2-8、超时设置" class="headerlink" title="3.2.8、超时设置"></a>3.2.8、超时设置</h4><p>　　在本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才可能会收到一个响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，我们应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错。</p>
<p>　　设置超时时间需要用到 timeout参数。这个时间的计算是发出 Request 到服务器返回 Response 的时间。</p>
<p>　　下面用一个实例来感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">"https://www.taobao.com"</span>, timeout = <span class="number">1</span>)</div><div class="line">print(r.status_code)</div></pre></td></tr></table></figure></p>
<p>　　通过这样的方式，我们可以将超时时间设置为 1 秒，如果 1 秒内没有响应，那就抛出异常。</p>
<p>　　实际上请求分为两个阶段，即 connect（连接）和 read（读取）。</p>
<p>　　上面的设置 timeout 值将会用作 connect 和 read 二者的 timeout 总和。</p>
<p>　　如果要分别指定，就可以传入一个元组：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">r = requests.get(<span class="string">'https://www.taobao.com'</span>, timeout=(<span class="number">5</span>,<span class="number">11</span>, <span class="number">30</span>))</div></pre></td></tr></table></figure></p>
<p>　　如果想永久等待，那么我们可以直接将 timeout 设置为 None，或者不设置直接留空，因为默认是 None。这样的话，如果服务器还在运行，但是响应特别慢，那就慢慢等吧，它永远不会返回超时错误的。</p>
<h4 id="3-2-9、身份认证"><a href="#3-2-9、身份认证" class="headerlink" title="3.2.9、身份认证"></a>3.2.9、身份认证</h4><p>　　如果遇到这样的网站验证，可以使用 Requests 自带的身份认证功能，实例如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> requests.auth <span class="keyword">import</span> HTTPBasicAuth</div><div class="line"></div><div class="line">r = requests.get(<span class="string">'http://localhost:5000'</span>, auth=HTTPBasicAuth(<span class="string">'username'</span>, <span class="string">'password'</span>))</div><div class="line">print(r.status_code)</div></pre></td></tr></table></figure></p>
<p>　　如果用户名和密码正确的话，请求时就会自动认证成功，会返回 200 状态码，如果认证失败，则会返回 401 状态码。</p>
<p>　　当然如果参数都传一个 HTTPBasicAuth 类，就显得有点繁琐了，所以 Requests 提供了一个更简单的写法，可以直接传一个元组，它会默认使用 HTTPBasicAuth 这个类来认证。</p>
<p>　　所以上面的代码可以直接简写如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">'http://localhost:5000'</span>, auth=(<span class="string">'username'</span>, <span class="string">'password'</span>))</div><div class="line">print(r.status_code)</div></pre></td></tr></table></figure></p>
<p>　　运行效果和上面的是一样的。</p>
<p>　　Requests 还提供了其他的认证方式，如 OAuth 认证，不过需要安装 oauth 包，命令如下：</p>
<p>　　使用 OAuth1 认证的方法如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> requests_oauthlib <span class="keyword">import</span> OAuth1</div><div class="line"></div><div class="line">url = <span class="string">'https://api.twitter.com/1.1/account/verify_credentials.json'</span></div><div class="line">auth = OAuth1(<span class="string">'YOUR_APP_KEY'</span>, <span class="string">'YOUR_APP_SECRET'</span>,</div><div class="line">              <span class="string">'USER_OAUTH_TOKEN'</span>, <span class="string">'USER_OAUTH_TOKEN_SECRET'</span>)</div><div class="line">requests.get(url, auth=auth)</div></pre></td></tr></table></figure></p>
<h4 id="3-2-10、Prepared-Request"><a href="#3-2-10、Prepared-Request" class="headerlink" title="3.2.10、Prepared Request"></a>3.2.10、Prepared Request</h4><p>　　在前面介绍 Urllib 时我们可以将 Request 表示为一个数据结构，Request 的各个参数都可以通过一个 Request 对象来表示，在Requests 里面同样可以做到，这个数据结构就叫 Prepared Request。</p>
<p>　　我们用一个实例感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</div><div class="line"></div><div class="line">url = <span class="string">'http://httpbin.org/post'</span></div><div class="line">data = &#123;</div><div class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span></div><div class="line">&#125;</div><div class="line">headers = &#123;</div><div class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span></div><div class="line">&#125;</div><div class="line">s = Session()</div><div class="line">req = Request(<span class="string">'POST'</span>, url, data=data, headers=headers)</div><div class="line">prepped = s.prepare_request(req)</div><div class="line">r = s.send(prepped)</div><div class="line">print(r.text)</div></pre></td></tr></table></figure></p>
<p>　　在这里我们引入了 Request，然后用 url、data、headers 参数构造了一个 Request 对象，这时我们需要再调用 Session 的 prepare_request() 方法将其转换为一个 Prepared Request 对象，然后调用 send() 方法发送即可</p>
<hr>
<h2 id="3-3、正则表达式"><a href="#3-3、正则表达式" class="headerlink" title="3.3、正则表达式"></a>3.3、正则表达式</h2><p>　　我们打开开源中国提供的正则表达式测试工具：<a href="http://tool.oschina.net/regex/，打开之后我们可以输入待匹配的文本，然后选择常用的正则表达式，就可以从我们输入的文本中得出相应的匹配结果了。" target="_blank" rel="external">http://tool.oschina.net/regex/，打开之后我们可以输入待匹配的文本，然后选择常用的正则表达式，就可以从我们输入的文本中得出相应的匹配结果了。</a></p>
<h4 id="3-3-1-re库"><a href="#3-3-1-re库" class="headerlink" title="3.3.1 re库"></a>3.3.1 re库</h4><p>　　Python 的 re 库提供了整个正则表达式的实现，利用 re 库我们就可以在 Python 中使用正则表达式了，在 Python 中写正则表达式几乎都是用的这个库，下面我们就来了解下它的一些常用方法。</p>
<p><strong>match()</strong></p>
<p>　　在这里首先介绍第一个常用的匹配方法，match() 方法，我们向这个方法传入要匹配的字符串以及正则表达式，就可以来检测这个正则表达式是否该匹配字符串了。</p>
<p>　　match() 方法会尝试从字符串的起始位置匹配正则表达式，如果匹配，就返回匹配成功的结果，如果不匹配，那就返回 None。</p>
<p>　　我们用一个实例来感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">content = <span class="string">'Hello 123 4567 World_This is a Regex Demo'</span></div><div class="line">print(len(content))</div><div class="line">result = re.match(<span class="string">'^Hello\s\d\d\d\s\d&#123;4&#125;\s\w&#123;10&#125;'</span>, content)</div></pre></td></tr></table></figure></p>
<p>　　我们调用 match() 方法，第一个参数传入了正则表达式，第二个参数传入了要匹配的字符串。</p>
<p>　　打印输出一下结果，可以看到结果是 SRE_Match 对象，证明成功匹配，它有两个方法，group() 方法可以输出匹配到的内容，结果是 Hello 123 4567 World_This，这恰好是我们正则表达式规则所匹配的内容，span() 方法可以输出匹配的范围，结果是 (0, 25)，这个就是匹配到的结果字符串在原字符串中的位置范围</p>
<p><strong>匹配目标</strong></p>
<p>　　刚才我们用了 match() 方法可以得到匹配到的字符串内容，但是如果我们想从字符串中提取一部分内容怎么办呢？就像最前面的实例一样，从一段文本中提取出邮件或电话号等内容。</p>
<p>　　在这里可以使用 () 括号来将我们想提取的子字符串括起来，() 实际上就是标记了一个子表达式的开始和结束位置，被标记的每个子表达式会依次对应每一个分组，我们可以调用 group() 方法传入分组的索引即可获取提取的结果。</p>
<p>　　下面我们用一个实例感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">content = <span class="string">'Hello 1234567 World_This is a Regex Demo'</span></div><div class="line">result = re.match(<span class="string">'^Hello\s(\d+)\sWorld'</span>, content)</div><div class="line">print(result)</div><div class="line">print(result.group())</div><div class="line">print(result.group(<span class="number">1</span>))</div><div class="line">print(result.span())</div></pre></td></tr></table></figure></p>
<p>　　依然是前面的字符串，在这里我们想匹配这个字符串并且把其中的 1234567 提取出来，在这里我们将数字部分的正则表达式用 () 括起来，然后接下来调用了group(1) 获取匹配结果。</p>
<p>　　运行结果如下：</p>
<pre><code>&lt;_sre.SRE_Match object; span=(0, 19), match=&apos;Hello 1234567 World&apos;&gt;
Hello 1234567 World
1234567
(0, 19)
</code></pre><p>　　可以看到在结果中成功得到了 1234567，我们获取用的是group(1)，与 group() 有所不同，group() 会输出完整的匹配结果，而 group(1) 会输出第一个被 () 包围的匹配结果，假如正则表达式后面还有 () 包括的内容，那么我们可以依次用 group(2)、group(3) 等来依次获取。</p>
<p><strong>通用匹配</strong></p>
<p>　　刚才我们写的正则表达式其实比较复杂，出现空白字符我们就写 \s 匹配空白字符，出现数字我们就写 \d 匹配数字，工作量非常大，其实完全没必要这么做，还有一个万能匹配可以用，也就是 .<em> （点星），.（点）可以匹配任意字符（除换行符），</em>（星） 又代表匹配前面的字符无限次，所以它们组合在一起就可以匹配任意的字符了，有了它我们就不用挨个字符地匹配了。</p>
<p><strong>贪婪与非贪婪</strong></p>
<p>　　贪婪匹配下，.<em> 会匹配尽可能多的字符，我们的正则表达式中 .</em> 后面是 \d+，也就是至少一个数字，并没有指定具体多少个数字，所以 .* 就尽可能匹配多的字符，所以它把 123456 也匹配了，给 \d+ 留下一个可满足条件的数字 7，所以 \d+ 得到的内容就只有数字 7 了。</p>
<p>　　但这样很明显会给我们的匹配带来很大的不便，有时候匹配结果会莫名其妙少了一部分内容。其实这里我们只需要使用非贪婪匹配匹配就好了，非贪婪匹配的写法是 .*?</p>
<p><strong>修饰符</strong></p>
<p>　　正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。</p>
<p>　　我们用一个实例先来感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">content = <span class="string">'''Hello 1234567 World_This</span></div><div class="line">is a Regex Demo</div><div class="line">'''</div><div class="line">result = re.match(<span class="string">'^He.*?(\d+).*?Demo$'</span>, content)</div><div class="line">print(result.group(<span class="number">1</span>))</div></pre></td></tr></table></figure></p>
<p>　　和上面的例子相仿，我们在字符串中加了个换行符，正则表达式也是一样的来匹配其中的数字，看一下运行结果：</p>
<pre><code>AttributeError Traceback (most recent call last)
&lt;ipython-input-18-c7d232b39645&gt; in &lt;module&gt;()
      5 &apos;&apos;&apos;
      6 result = re.match(&apos;^He.*?(\d+).*?Demo$&apos;, content)
----&gt; 7 print(result.group(1))

AttributeError: &apos;NoneType&apos; object has no attribute &apos;group&apos;
</code></pre><p>　　运行直接报错，也就是说正则表达式没有匹配到这个字符串，返回结果为 None，而我们又调用了 group() 方法所以导致AttributeError。</p>
<p>　　那我们加了一个换行符为什么就匹配不到了呢？是因为 . 匹配的是除换行符之外的任意字符，当遇到换行符时，.*? 就不能匹配了，所以导致匹配失败。</p>
<p>　　那么在这里我们只需要加一个修饰符 re.S，即可修正这个错误。</p>
<pre><code>result = re.match(&apos;^He.*?(\d+).*?Demo$&apos;, content, re.S)
</code></pre><p>　　在 match() 方法的第三个参数传入 re.S，它的作用是使 . 匹配包括换行符在内的所有字符。</p>
<p>　　运行结果：</p>
<pre><code>1234567
</code></pre><p>　　这个 re.S 在网页匹配中会经常用到，因为 HTML 节点经常会有换行，加上它我们就可以匹配节点与节点之间的换行了。</p>
<ul>
<li>re.I：使匹配对大小写不敏感</li>
<li>re.L：做本地化识别（locale-aware）匹配</li>
<li>re.M：多行匹配，影响 ^ 和 $</li>
<li>re.S：使 . 匹配包括换行在内的所有字符</li>
<li>re.U：根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B.</li>
<li>re.X：该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。</li>
</ul>
<p><strong>转义匹配</strong></p>
<p>　　我们知道正则表达式定义了许多匹配模式，如 . 匹配除换行符以外的任意字符，但是如果目标字符串里面它就包含 . 我们改怎么匹配？</p>
<p>　　那么这里就需要用到转义匹配了，我们用一个实例来感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">content = <span class="string">'(百度)www.baidu.com'</span></div><div class="line">result = re.match(<span class="string">'\(百度\)www\.baidu\.com'</span>, content)</div><div class="line">print(result)</div></pre></td></tr></table></figure></p>
<p><strong>search()</strong></p>
<p>　　我们在前面提到过 match() 方法是从字符串的开头开始匹配，一旦开头不匹配，那么整个匹配就失败了。</p>
<p>　　所以 match() 方法在我们在使用的时候需要考虑到开头的内容，所以在做匹配的时候并不那么方便，它适合来检测某个字符串是否符合某个正则表达式的规则。</p>
<p>　　所以在这里就有另外一个方法 search()，它在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果，也就是说，正则表达式可以是字符串的一部分，在匹配时，search() 方法会依次扫描字符串，直到找到第一个符合规则的字符串，然后返回匹配内容，如果搜索完了还没有找到，那就返回 None。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">result = re.search(<span class="string">'&lt;li.*?active.*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>, html, re.S)</div><div class="line"><span class="keyword">if</span> result:</div><div class="line">    print(result.group(<span class="number">1</span>), result.group(<span class="number">2</span>))</div></pre></td></tr></table></figure></p>
<p>　　由于我们需要获取的歌手和歌名都已经用了小括号包围，所以可以用 group() 方法获取，序号依次对应 group() 的参数。</p>
<p><strong>findall()</strong></p>
<p>　　在前面我们说了 search() 方法的用法，它可以返回匹配正则表达式的第一个内容，但是如果我们想要获取匹配正则表达式的所有内容的话怎么办？这时就需要借助于 findall() 方法了。</p>
<p>　　findall() 方法会搜索整个字符串然后返回匹配正则表达式的所有内容。</p>
<p>　　还是上面的 HTML 文本，如果我们想获取所有 a 节点的超链接、歌手和歌名，就可以将 search() 方法换成 findall() 方法。如果有返回结果的话就是列表类型，所以我们需要遍历一下来获依次获取每组内容。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">results = re.findall(<span class="string">'&lt;li.*?href="(.*?)".*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>, html, re.S)</div><div class="line">print(results)</div><div class="line">print(type(results))</div><div class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</div><div class="line">    print(result)</div><div class="line">    print(result[<span class="number">0</span>], result[<span class="number">1</span>], result[<span class="number">2</span>])</div></pre></td></tr></table></figure></p>
<p>　　运行结果：</p>
<pre><code>[(&apos;/2.mp3&apos;, &apos;任贤齐&apos;, &apos;沧海一声笑&apos;), (&apos;/3.mp3&apos;, &apos;齐秦&apos;, &apos;往事随风&apos;), (&apos;/4.mp3&apos;, &apos;beyond&apos;, &apos;光辉岁月&apos;), (&apos;/5.mp3&apos;, &apos;陈慧琳&apos;, &apos;记事本&apos;), (&apos;/6.mp3&apos;, &apos;邓丽君&apos;, &apos;但愿人长久&apos;)]
&lt;class &apos;list&apos;&gt;
(&apos;/2.mp3&apos;, &apos;任贤齐&apos;, &apos;沧海一声笑&apos;)
/2.mp3 任贤齐 沧海一声笑
(&apos;/3.mp3&apos;, &apos;齐秦&apos;, &apos;往事随风&apos;)
/3.mp3 齐秦 往事随风
(&apos;/4.mp3&apos;, &apos;beyond&apos;, &apos;光辉岁月&apos;)
/4.mp3 beyond 光辉岁月
(&apos;/5.mp3&apos;, &apos;陈慧琳&apos;, &apos;记事本&apos;)
/5.mp3 陈慧琳 记事本
(&apos;/6.mp3&apos;, &apos;邓丽君&apos;, &apos;但愿人长久&apos;)
/6.mp3 邓丽君 但愿人长久
</code></pre><p>　　可以看到，返回的列表的每个元素都是元组类型，我们用对应的索引依次取出即可。</p>
<p>　　所以，如果只是获取第一个内容，可以用 search() 方法，当需要提取多个内容时，就可以用 findall() 方法</p>
<p><strong>sub()</strong></p>
<p>　　正则表达式除了提取信息，我们有时候还需要借助于它来修改文本，比如我们想要把一串文本中的所有数字都去掉，如果我们只用字符串的 replace() 方法那就太繁琐了，在这里我们就可以借助于 sub() 方法。</p>
<p>　　我们用一个实例来感受一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">content = <span class="string">'54aK54yr5oiR54ix5L2g'</span></div><div class="line">content = re.sub(<span class="string">'\d+'</span>, <span class="string">''</span>, content)</div><div class="line">print(content)</div></pre></td></tr></table></figure></p>
<p>　　运行结果：</p>
<pre><code>aKyroiRixLg
</code></pre><p><strong>compile()</strong></p>
<p>　　前面我们所讲的方法都是用来处理字符串的方法，最后再介绍一个 compile() 方法，这个方法可以讲正则字符串编译成正则表达式对象，以便于在后面的匹配中复用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">content1 = <span class="string">'2016-12-15 12:00'</span></div><div class="line">content2 = <span class="string">'2016-12-17 12:55'</span></div><div class="line">content3 = <span class="string">'2016-12-22 13:21'</span></div><div class="line">pattern = re.compile(<span class="string">'\d&#123;2&#125;:\d&#123;2&#125;'</span>)</div><div class="line">result1 = re.sub(pattern, <span class="string">''</span>, content1)</div><div class="line">result2 = re.sub(pattern, <span class="string">''</span>, content2)</div><div class="line">result3 = re.sub(pattern, <span class="string">''</span>, content3)</div><div class="line">print(result1, result2, result3)</div></pre></td></tr></table></figure></p>
<p>　　例如这里有三个日期，我们想分别将三个日期中的时间去掉，所以在这里我们可以借助于 sub() 方法，sub() 方法的第一个参数是正则表达式，但是这里我们没有必要重复写三个同样的正则表达式，所以可以借助于 compile() 方法将正则表达式编译成一个正则表达式对象，以便复用。</p>
<p>　　运行结果：</p>
<pre><code>2016-12-15  2016-12-17  2016-12-22
</code></pre><p>　　<br>　　另外 compile() 还可以传入修饰符，例如 re.S 等修饰符，这样在 search()、findall() 等方法中就不需要额外传了。所以 compile() 方法可以说是给正则表达式做了一层封装，以便于我们更好地复用。</p>

            <div class="clearfix"></div>
            <hr class="nogutter">
        </div>
        <nav class="pagination" role="pagination">
    
    <a class="pull-left" href="/HenryBalthier.github.io/2017/08/31/【爬虫系列】4、解析库和数据库/" style="float: left;">
        ← 【爬虫系列】4、解析库和数据库
    </a>
    
    
    <a class="pull-right" href="/HenryBalthier.github.io/2017/08/29/【爬虫系列】2、爬虫基础/">
        【爬虫系列】2、爬虫基础 →
    </a>
    
</nav>

        <div class="duoshuo">


</div>
    </div>
</section>


      
<!-- ============================ Footer =========================== -->

<footer>
    <div class="container">
            <div class="copy">
                <p>
                    &copy; 2014<script>new Date().getFullYear()>2010&&document.write("-"+new Date().getFullYear());</script>, Content By Yu Fangheng. All Rights Reserved.
                </p>
                <p>Theme By <a href="//go.kieran.top" style="color: #767D84">Kieran</a></p>
            </div>
            <div class="social">
                <ul>
                    
                    <li><a href="https://github.com/" title="Github" target="_blank"><i class="icon-github"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://twitter.com/" title="Twitter" target="_blank"><i class="icon-twitter"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://www.facebook.com/" title="Facebook" target="_blank"><i class="icon-facebook"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://google.com/" title="Google-Plus" target="_blank"><i class="icon-google-plus"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="http://weibo.com/" title="Sina-Weibo" target="_blank"><i class="icon-sina-weibo"></i></a>&nbsp;</li>
                    
                </ul>
            </div>
            <div class="clearfix"> </div>
        </div>
</footer>

<!-- ============================ END Footer =========================== -->
      <!-- Load our scripts -->
<!-- Resizable 'on-demand' full-height hero -->
<script type="text/javascript">
    var resizeHero = function () {
        var hero = $(".cover,.heightblock"),
            window1 = $(window);
        hero.css({
            "height": window1.height()
        });
    };

    resizeHero();

    $(window).resize(function () {
        resizeHero();
    });
</script>
<script src="/HenryBalthier.github.io/js/plugins.min.js"></script><!-- Bootstrap core and concatenated plugins always load here -->
<script src="/HenryBalthier.github.io/js/jquery.flexslider-min.js"></script><!-- Flexslider plugin -->
<script src="/HenryBalthier.github.io/js/scripts.js"></script><!-- Theme scripts -->


<link rel="stylesheet" href="/HenryBalthier.github.io/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/HenryBalthier.github.io/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$('#intro').find('img').each(function(){
  var alt = this.alt;

  if (alt){
    $(this).after('<span class="caption" style="display:none">' + alt + '</span>');
  }

  $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="gallery" />');
});
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

<!-- Initiate flexslider plugin -->
<script type="text/javascript">
    $(document).ready(function($) {
      (function(){
        console.log('font');
        var getCss = function(path) {
          var head = document.getElementsByTagName('head')[0];
          link = document.createElement('link');
          link.href = path;
          link.rel = 'stylesheet';
          link.type = 'text/css';
          head.appendChild(link);
        };
        getCss('https://fonts.googleapis.com/css?family=Montserrat:400,700');
        getCss('https://fonts.googleapis.com/css?family=Open+Sans:400,600');
      })();
      $('.flexslider').flexslider({
        animation: "fade",
        prevText: "",
        nextText: "",
        directionNav: true
      });
    });
</script>

</body>
</html>
